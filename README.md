**Task 1: Convolutional Neural Networks (CIFAR10 Classification)**

  We developed a classification network for the CIFAR10 dataset.
    The goal was to achieve a final accuracy on the test set greater than 80%.
    We needed to ensure that the number of trainable parameters (weights) in the model remained below 50,000.
    We experimented with various model architectures, hyperparameters (learning rate, batch size), and network configurations to optimize performance.
    We provided a detailed model architecture description and training procedure, including any data augmentation techniques employed.
    We created convergence graphs illustrating error and loss as functions of time (epochs) for both training and test performance.
    We summarized our attempts and drew conclusions based on the results obtained.
    Our best test error, achieved by the submitted model, was explicitly mentioned in the report.

**Task 2: Names Generation (Character-level Language Model)**

  In this task, we trained a character-level language model to generate names based on origin country and first letter inputs.
  We needed to specify how the input data was represented and determine the prediction strategy.
  We generated 5 examples of names using the trained model and provided the corresponding inputs.
  We described the architecture and training procedure for the language model, including any data preprocessing steps.
  We created convergence graphs for error and loss as functions of time (epochs) for both training and test performance.
  We summarized our attempts and shared conclusions based on the results obtained.
  We also included the best test error achieved by the submitted model.
